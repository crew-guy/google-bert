{"cells":[{"cell_type":"markdown","metadata":{"id":"UhFa7ZhKqaWm"},"source":["### Stage 1 : Importing dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JVqg36CnqNxM"},"outputs":[],"source":["import numpy as np\n","import math\n","\n","# model that allows us to do text-process\n","import re\n","\n","# for extracting csv datasets into usable data frames\n","import pandas as pd\n","\n","# parse xml/html form of data \u0026 decode it into usable data frame\n","from bs4 import BeautifulSoup\n","\n","#random needed during data processing\n","import random\n","\n","# get data from personal google drive\n","from google.colab import drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11935,"status":"ok","timestamp":1653338843763,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"Unvem5ovq8uS","outputId":"2b893cee-f13b-48ab-c6e6-fdad60b3a5c9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bert-for-tf2\n","  Downloading bert-for-tf2-0.14.9.tar.gz (41 kB)\n","\u001b[K     |████████████████████████████████| 41 kB 128 kB/s \n","\u001b[?25hCollecting py-params\u003e=0.9.6\n","  Downloading py-params-0.10.2.tar.gz (7.4 kB)\n","Collecting params-flow\u003e=0.8.0\n","  Downloading params-flow-0.8.2.tar.gz (22 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow\u003e=0.8.0-\u003ebert-for-tf2) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow\u003e=0.8.0-\u003ebert-for-tf2) (4.64.0)\n","Building wheels for collected packages: bert-for-tf2, params-flow, py-params\n","  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-py3-none-any.whl size=30535 sha256=13c439d61e1e5d2e0d2b7e5e368d7fb6958bb7a3dfffcd76322285eaee17fe1a\n","  Stored in directory: /root/.cache/pip/wheels/47/b6/e5/8c76ec779f54bc5c2f1b57d2200bb9c77616da83873e8acb53\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.8.2-py3-none-any.whl size=19472 sha256=12e388ce7f195f862acf066fae43c85354ee46616fd953c6ed28763add01dbb7\n","  Stored in directory: /root/.cache/pip/wheels/0e/fc/d2/a44fff33af0f233d7def6e7de413006d57c10e10ad736fe8f5\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.10.2-py3-none-any.whl size=7911 sha256=d7cf177b89f26f22390169c7592360cdc4f74362504e41e4c4ad5af60e954c2e\n","  Stored in directory: /root/.cache/pip/wheels/e1/11/67/33cc51bbee127cb8fb2ba549cd29109b2f22da43ddf9969716\n","Successfully built bert-for-tf2 params-flow py-params\n","Installing collected packages: py-params, params-flow, bert-for-tf2\n","Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 15.7 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n"]}],"source":["# useful sdk for user-friendly usage of google's official package\n","!pip install bert-for-tf2\n","\n","# required by bert-for-tf2 for decoding\n","!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p4V_fs_IrHXq"},"outputs":[],"source":["try:\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","\n","import tensorflow as tf\n","\n","# platform where lotta ML models are upload (for downloading weights of BERT)\n","import tensorflow_hub as hub\n","\n","# building layers for our CNNs\n","from tensorflow.keras import layers\n","import bert"]},{"cell_type":"markdown","metadata":{"id":"nCEKnGacto3q"},"source":["## Stage 2 : Data processing\n","\n","We import files from our Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39405,"status":"ok","timestamp":1653339956634,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"UIZx_XgxtvRv","outputId":"190124a4-3d83-4f80-9755-6d3fe983917e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqQiaKi3vQUh"},"outputs":[],"source":["cols = [\"sentiment\", 'id', 'date', 'query', 'user', 'text']\n","data = pd.read_csv(\n","    \"/content/drive/MyDrive/Colab Notebooks/BERT Tokenizer/training.1600000.processed.noemoticon.csv\",\n","    header=None,\n","    names=cols,\n","    engine=\"python\",\n","    encoding=\"latin1\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fC1zgyE_wdth"},"outputs":[],"source":["data.drop([\"id\",\"date\",\"query\",\"user\"], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1653341820584,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"F3h3YXvUxpZh","outputId":"26a6496b-cf98-4ca1-b596-016606308291"},"outputs":[{"data":{"text/html":["\n","  \u003cdiv id=\"df-5daa4221-fe0a-44c1-b69e-4424480e64d8\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003esentiment\u003c/th\u003e\n","      \u003cth\u003etext\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e@switchfoot http://twitpic.com/2y1zl - Awww, t...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003eis upset that he can't update his Facebook by ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e@Kenichan I dived many times for the ball. Man...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003emy whole body feels itchy and like its on fire\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003e0\u003c/td\u003e\n","      \u003ctd\u003e@nationwideclass no, it's not behaving at all....\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5daa4221-fe0a-44c1-b69e-4424480e64d8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","        \n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","      \n","  \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-5daa4221-fe0a-44c1-b69e-4424480e64d8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5daa4221-fe0a-44c1-b69e-4424480e64d8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n","  "],"text/plain":["   sentiment                                               text\n","0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1          0  is upset that he can't update his Facebook by ...\n","2          0  @Kenichan I dived many times for the ball. Man...\n","3          0    my whole body feels itchy and like its on fire \n","4          0  @nationwideclass no, it's not behaving at all...."]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["data.head(5)"]},{"cell_type":"markdown","metadata":{"id":"BBLZreHXxb4i"},"source":["## Stage 3 : Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"rNIMDWhdxtf3"},"source":["### Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6E9m6RRxynx"},"outputs":[],"source":["def clean_tweet(tweet):\n","    tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n","\n","    # for replacing certain string regex patterns with desired values\n","    tweet = re.sub(r\"@[a-zA-Z0-9]+\",\" \", tweet)\n","    tweet = re.sub(r\"https?://[a-zA-Z0-9./]+\",\" \", tweet)\n","    tweet = re.sub(r\"[^a-zA-Z.!?']\", \" \", tweet)\n","    tweet = re.sub(r\" +\", \" \", tweet)\n","    return tweet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lpvcvZg3QDA"},"outputs":[],"source":["data_clean = [clean_tweet(tweet) for tweet in data.text]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fbr5DU113Ymp"},"outputs":[],"source":["data_labels = data.sentiment.values\n","data_labels[data_labels == 4] = 1"]},{"cell_type":"markdown","metadata":{"id":"WQRDz72-31jT"},"source":["### Tokenization\n","\n","We need to create a BERT layer to have access to the metadata for the tokenizer (such as vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7A5d49xW4_QV"},"outputs":[],"source":["FullTokenizer = bert.bert_tokenization.FullTokenizer\n","\n","# we use trainable as false since we only want to get the info the tokenizer of BERT, not fine-tune its weights at all\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False )\n","\n","# for getting the vocabulary file for the tokenizer\n","vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = FullTokenizer(vocab_file, do_lower_case)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":596,"status":"ok","timestamp":1653343111745,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"m4WsNyZL7RoI","outputId":"a703c92f-9d32-44ee-eaae-43f35c05c285"},"outputs":[{"data":{"text/plain":["['my', 'dog', 'loves', 'straw', '##berries', '.']"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.tokenize(\"My dog loves strawberries.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":714,"status":"ok","timestamp":1653343205422,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"xiNFmqtF7cNY","outputId":"674ac413-32f0-4cf4-8762-027b5f15302e"},"outputs":[{"data":{"text/plain":["[2026, 3899, 7459, 13137, 20968, 1012]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"My dog loves strawberries.\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dzDzpKUd7zBH"},"outputs":[],"source":["def encode_sentence(sent):\n","    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mR-veZ5W79Oh"},"outputs":[],"source":["data_inputs = [encode_sentence(sent) for sent in data_clean]"]},{"cell_type":"markdown","metadata":{"id":"FoebgPWu8FpQ"},"source":["### Dataset Creation\n","\n","We will create padded batches (so we pad sentences for each batch independently), this way we will create minimum number of padding tokens possible. For that, we sort the sentences by length, apply padded_batches \u0026 then shuffle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRSMljr58tvC"},"outputs":[],"source":["data_with_len = [[sent, data_labels[i], len(sent)] for i, sent in enumerate(data_inputs)]\n","random.shuffle(data_with_len)\n","data_with_len.sort(key=lambda x:x[2])\n","\n","# basically, only include sentences whose length \u003e 7\n","sorted_all = [(sent_lab[0], sent_lab[1]) for sent_lab in data_with_len if sent_lab[2] \u003e 7]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UJBM6ZK0CvA9"},"outputs":[],"source":["# we currently have all our pairs of tweets \u0026 labels, that are cleaned, sorted \u0026 filtered by sentence length\n","# usually we use the from-tensor slices for this purpose - refer - https://www.tensorflow.org/guide/tensor_slicing\n","# here, we can't do it since our sentences are of different length\n","# so we need to call the from-generator -\u003e so we can get datasets that are of different length, but we need to give it a generator \n","# a generator is something that just gives, one after the other\n","# write a lambda func, that passes our input datasets, one-by-one to from-generator\n","all_dataset = tf.data.Dataset.from_generator(lambda: sorted_all, output_types =(tf.int32, tf.int32))"]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1653346624008,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"q00ToXk3E4ZU","outputId":"37d901e4-36fe-47b1-83cf-828dbbc3cf6a"},"outputs":[{"data":{"text/plain":["(\u003ctf.Tensor: shape=(8,), dtype=int32, numpy=array([2821,  999, 4485,  999, 2773,  999, 4067, 2017], dtype=int32)\u003e,\n"," \u003ctf.Tensor: shape=(), dtype=int32, numpy=4\u003e)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(all_dataset))"]},{"cell_type":"markdown","metadata":{"id":"FXiF59hvFTr8"},"source":["Array of input dataset ( eg : [2821,  999, 4485,  999, 2773,  999, 4067, 2017]) \u0026 corresponding label (eg : 4)"]},{"cell_type":"markdown","metadata":{"id":"nHVOf7DyFt6Y"},"source":["**Padding**  - Padding phase is done at the same time as the batching phase"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":485,"status":"ok","timestamp":1653346635513,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"X9x1lS--GAnm"},"outputs":[],"source":["BATCH_SIZE = 32\n","all_batched = all_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ) ()))"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1653346668669,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"KAJlPPGBGpws","outputId":"3a7a146f-4f8e-4698-ffe9-e682efd6ed5c"},"outputs":[{"data":{"text/plain":["(\u003ctf.Tensor: shape=(32, 8), dtype=int32, numpy=\n"," array([[ 2821,   999,  4485,   999,  2773,   999,  4067,  2017],\n","        [ 2893,  3201,  2000,  2175,  2067,  2000,  2082,  1012],\n","        [ 1045,  1005,  1049,  2085,  2206, 16299,  2006, 10474],\n","        [ 2428,  2123,  1005,  1056,  2215, 17776,  1042,  2497],\n","        [10047,  2863,  2031,  3524,  2963,  2009,  6229,  6928],\n","        [ 1045,  3984,  1045,  2031, 16021,  5358,  6200,  3892],\n","        [ 1045,  3246,  2017,  2064,  2031,  1037,  2204,  2717],\n","        [ 4019,  1996,  6580,  8146,  1061,  5506,  4632,  2299],\n","        [ 4931,  2045,  2015,  2498,  3308,  2007,  2008,  1012],\n","        [ 2049,  1037,  3835, 11559,  4633,  1999, 14022,  2651],\n","        [ 3835,   999,  2008,  1005,  1055, 12476,  1012, 23156],\n","        [ 2003,  4634,  2061,  2524,  2005,  2026,  6429,  2611],\n","        [ 2012,  2658,  2458,  1012,  1012,  1012,  1012,  1012],\n","        [ 1045,  1005,  1049,  2061,  3407,  2005, 18431,  1012],\n","        [23987,  2813, 23298,  2003,  2025,  4569,  2012,  2035],\n","        [ 2021,  1045,  2180,  2102,  2022,  2012,  7348,  7395],\n","        [ 2074, 10720, 23111, 10695,  1042,  2080, 11895, 17644],\n","        [ 2021,  2017,  1005,  2128,  2069,  2006, 11943,  2660],\n","        [ 7632,  2045,  3246, 24471,  2092,  4283,  2005,  2206],\n","        [ 2031,  1037,  2307,  4465,  2007,  3566,  1998,  3611],\n","        [ 2042,  2045,  2589,  2008,  1012,  1045,  6639,  3041],\n","        [ 2026, 13451,  2567,  2003,  2488,  2559,  2084,  2033],\n","        [ 2085,  2008,  2428,  2003,  1037,  6209,   999,   999],\n","        [ 9464,  2005,  1996,  2154,  2100,  2100,  2100,   999],\n","        [12459,  3246,  2115, 10627,  2024,  3876,  1012,  1012],\n","        [16215,  2595,  2172, 24761,  2005,  2008,  1054,  1056],\n","        [ 4067,  2017,  2893,  2109,  2000,  2009,  3243,  3435],\n","        [ 2129,  2272,  1045,  2347,  2102,  4778,   999,   999],\n","        [ 3407,  2388,  1005,  1055,  2154,   999,  2420,   999],\n","        [ 2293,  2026,  8771,  2490,  2136, 11265,  4103,   999],\n","        [ 2025,  2559,  2830,  2000,  2082,  1999,  1996,  2851],\n","        [ 7592,   999,  2064,  1045,  2022,  1997,  5375,  1029]],\n","       dtype=int32)\u003e, \u003ctf.Tensor: shape=(32,), dtype=int32, numpy=\n"," array([4, 0, 4, 0, 0, 0, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 4, 0, 4, 4, 0, 0,\n","        4, 4, 0, 4, 4, 0, 4, 4, 0, 4], dtype=int32)\u003e)"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["next(iter(all_batched))"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":561,"status":"ok","timestamp":1653347115388,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"ELslU7xAJAtq"},"outputs":[],"source":["NB_BATCHES = math.ceil(len(sorted_all) / BATCH_SIZE)\n","NB_BATCHES_TEST = NB_BATCHES//10\n","\n","# passing buffer size of shuffling = number of batches. This solves the problem on solely training/testing on 1st few examples i.e. examples with sentence length as say 5;\n","all_batched.shuffle(NB_BATCHES)\n","test_dataset = all_batched.take(NB_BATCHES_TEST)\n","train_dataset = all_batched.skip(NB_BATCHES_TEST)"]},{"cell_type":"markdown","metadata":{"id":"5s-7Au0bKykM"},"source":["## Stage 3 : Model Building"]},{"cell_type":"code","execution_count":78,"metadata":{"executionInfo":{"elapsed":464,"status":"ok","timestamp":1653355424764,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"FlfWd2NBL7kv"},"outputs":[],"source":["class DCNN(tf.keras.Model):\n","    def __init__(self,\n","                vocab_size, # for applying embedding layer for our CNN\n","                emb_dim=120, # for using our vectors to build \u0026 traing our CNN here\n","                nb_filters=50, # number of convolutional filters for each size. we will get 50 feature detectors of size 2, 3 \u0026 4 each\n","                FFN_units=12,  # number of hidden units we will use in our dense layers at the end becuase the end of the last feed forward neural network parts of our CNN will be made of 2 dense layers, having a number of hidden units in between them\n","                nb_classes=2,\n","                dropout_rate=0.1,\n","                training=False,# we need to know whether we are in \"training\" phase or not, else we won't use dropout_rate\n","                name=\"dcnn\"\n","                ):\n","        super(DCNN, self).__init__(name=name)\n","\n","        ## Embedding layer\n","        ## each word is a number right now. We wish to take these input tokens \u0026 convert them into vectors\n","        #  also, the parameters/weights will be trained here\n","        self.embedding = layers.Embedding(vocab_size, emb_dim)\n","\n","        ## focusses on 2 consecutive words\n","        ## Currently, shifting feature detectors in only 1 dimension\n","        self.bigram = layers.Conv1D(filters=nb_filters,\n","                                    kernel_size=2,\n","                                    # when you have strides of more than 1, sometimes the last iteration of our feature detectors could get out of the\n","                                    # max range of our matrix of our sentence \u0026 this padding indicates how we handle it\n","                                    padding='valid',\n","                                    # add a function to each element of our feature map (output of conventional phase)\n","                                    # here, we basically just set all negative values to 0\n","                                    activation=\"relu\",\n","                                    )\n","        self.trigram = layers.Conv1D(filters=nb_filters,\n","                                    kernel_size=3,\n","                                    # when you have strides of more than 1, sometimes the last iteration of our feature detectors could get out of the\n","                                    # max range of our matrix of our sentence \u0026 this padding indicates how we handle it\n","                                    padding='valid',\n","                                    # add a function to each element of our feature map (output of conventional phase)\n","                                    # here, we basically just set all negative values to 0\n","                                    activation=\"relu\",\n","                                    )\n","        self.fourgram = layers.Conv1D(filters=nb_filters,\n","                                    kernel_size=4,\n","                                    # when you have strides of more than 1, sometimes the last iteration of our feature detectors could get out of the\n","                                    # max range of our matrix of our sentence \u0026 this padding indicates how we handle it\n","                                    padding='valid',\n","                                    # add a function to each element of our feature map (output of conventional phase)\n","                                    # here, we basically just set all negative values to 0\n","                                    activation=\"relu\",\n","                                    )\n","        \n","        ## Now we gotta create a layer i.e. function that  will take the max of those outputs, as we saw before\n","        self.pool = layers.GlobalMaxPool1D()\n","\n","        # Feed Forwards, Neural Network\n","        ## Need 2 dense layers with a hidden number of units between the 2 dense layers\n","        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n","\n","        ## dropout layer - shut down some certain neurons i.e. free up compute power to prevent the model from overfitting\n","        # each time, different neurons are shut off\n","        self.dropuout = layers.Dropout(rate=dropout_rate)\n","\n","        # \u003e0.5 =\u003e 1, else 0\n","        # if more than 2 classes, then number of output units = number of classs\n","        # for multiclass, we will use softmax activation so that we get probabilities for each class to be the answer \n","        if(nb_classes == 2):\n","            self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n","        else:\n","            self.last_dense = layers.Dense\n","\n","    # use our created neurons\n","    def call(self, inputs, training):\n","            x = self.embedding(inputs)\n","            x_1 = self.bigram(x)\n","            x_l = self.pool(x_1)\n","            x_2 = self.threegram(x)\n","            x_2 = self.pool(x_1)\n","            x_3 = self.fourgram(x)\n","            x_3 = self.pool(x_1) # (batch_size, nb_filters)\n","            \n","            merged = tf.concat([x_1, x_2, x_3], axis=1)\n","            nerged = self.dense_1(merged)\n","            merged = self.dropout(merged, training)\n","            output = self.last_dense(merged)\n","\n","            return output"]},{"cell_type":"markdown","metadata":{"id":"-Z_DPu22fUgF"},"source":["## Stage 4 : Training"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":769,"status":"ok","timestamp":1653355415671,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"0kxZuTIrfWEm"},"outputs":[],"source":["# Hyperparameters - parameters needed for training\n","\n","VOCAB_SIZE = len(tokenizer.vocab)\n","EMB_DIM = 200\n","NB_FILTERS = 100\n","FFN_UNITS = 256\n","NB_CLASSES = 2\n","\n","DROPOUT_RATE = 0.2\n","\n","NB_EPOCHS = 5"]},{"cell_type":"code","execution_count":79,"metadata":{"executionInfo":{"elapsed":449,"status":"ok","timestamp":1653355429896,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"c7boYnOwjb7d"},"outputs":[],"source":["Dcnn = DCNN(\n","    vocab_size=VOCAB_SIZE,\n","    nb_filters=NB_FILTERS,\n","    FFN_units=FFN_UNITS,\n","    emb_dim=EMB_DIM,\n","    dropout_rate=DROPOUT_RATE,\n",")"]},{"cell_type":"code","execution_count":82,"metadata":{"executionInfo":{"elapsed":860,"status":"ok","timestamp":1653355456320,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"tWhvUI1Aj46h"},"outputs":[],"source":["if NB_CLASSES == 2:\n","    Dcnn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n","else:\n","    Dcnn.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1653355023289,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"KZ6tZzOVkw17"},"outputs":[],"source":["checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/BERT Tokenizer/ckpt\"\n","\n","ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path,max_to_keep=1)\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.resotre(ckpt_manager.latest_checkpoint)\n","    print(\"Latest checkpoint restored \")\n"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":542,"status":"ok","timestamp":1653355436764,"user":{"displayName":"Ankit Sanghvi","userId":"00785673521333397915"},"user_tz":-330},"id":"rt7v-Y66l6Ok"},"outputs":[],"source":["# run some other misc. functions during the training\n","class MyCustomCallback(tf.keras.callbacks.Callback):\n","    def on_epoch_end(self, epoch, logs=None):\n","        ckpt_manager.save()\n","        print(\"Checkpoint is saved at{}\".format(checkpoint_path))"]},{"cell_type":"markdown","metadata":{"id":"YG4SO0Rtn4qU"},"source":["## Result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ZTJXLYFxn6gU"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","  37196/Unknown - 2200s 59ms/step - loss: -7279808610304.0000 - accuracy: 1.2602e-05Checkpoint is saved at/content/drive/MyDrive/Colab Notebooks/BERT Tokenizer/ckpt\n","37196/37196 [==============================] - 2200s 59ms/step - loss: -7279808610304.0000 - accuracy: 1.2602e-05\n","Epoch 2/5\n","24754/37196 [==================\u003e...........] - ETA: 11:35 - loss: -99221629829120.0000 - accuracy: 0.0000e+00"]}],"source":["Dcnn.fit(train_dataset,\n","         epochs=NB_EPOCHS,\n","         callbacks=[MyCustomCallback()])"]},{"cell_type":"markdown","metadata":{"id":"dj539EADpb9c"},"source":["## Evaluation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGN4rwT_pdk_"},"outputs":[],"source":["results = Dcnn.evaluate(test_dataset)\n","print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z6qwcxWepek3"},"outputs":[],"source":["def get_prediction(sentence):\n","\n","    # convert words from sentence into their respective numbers\n","    tokens = encode_sentence(sentence)\n","\n","    # convert the tokenized representation of words into a v\n","    inputs = tf.expand_dims(tokens, 0)\n","\n","    output = Dcnn(inputs, training=False)\n","\n","    sentiment = math.floor(output*2)\n","\n","    if sentiment == 0:\n","        print(\"Output of the model: {}\\nPredicted sentiment: negative.\".format(\n","            output))\n","    elif sentiment == 1:\n","        print(\"Output of the model: {}\\nPredicted sentiment: positive.\".format(\n","            output))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5zWwjb4ph1c"},"outputs":[],"source":["get_prediction(\"This movie was pretty interesting.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"64WvjTgbpjWE"},"outputs":[],"source":["get_prediction(\"I'd rather not do that again.\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPLZpOBYm/MPnUbQSk0c4AS","name":"BERT_Tokenizer","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}